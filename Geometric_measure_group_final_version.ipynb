{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YxgoRvcGk7T"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras import backend\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import math\n",
        "from math import log\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "from itertools import product\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8NeeWD-G5rP"
      },
      "source": [
        "Generate a Cantor Set, defined as for set $A_i$, $x=\\sum\\limits_{k=1}^\\infty{a_\\text{k}*(2^n)^\\text{-k}}$  where $a_\\text{k}\\in \\{ 0,2^\\text{i-1} \\}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcUxHginG3wQ",
        "outputId": "5c37b35b-a1df-4c18-8fd6-4d85489f60bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0078125, 0.03125, 0.0390625, 0.125, 0.1328125, 0.15625, 0.1640625, 0.5, 0.5078125, 0.53125, 0.5390625, 0.625, 0.6328125, 0.65625, 0.6640625]\n"
          ]
        }
      ],
      "source": [
        "x=[]\n",
        "kgap=4  # determines how many time the iteration will go, will generate 2^kgap numbers of points\n",
        "def xinAi(k,coor,power,dimension):  # for the case x in Ai, dimension is the dim of the cantor set, i=dimension, power starts from 1\n",
        "    for an in [0, 2**(dimension-1)]:\n",
        "        coor = coor + an*((2**dimension)**(-power))\n",
        "\n",
        "        if power < k:\n",
        "            xinAi(k, coor, power + 1, dimension)\n",
        "\n",
        "        else :\n",
        "            x.append(coor)\n",
        "\n",
        "xinAi(kgap,0,1,2)\n",
        "\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNMk8kO2OPR1"
      },
      "source": [
        "Several possible learning tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw7gwpPqOOyx"
      },
      "outputs": [],
      "source": [
        "def Distencefunction(HighDimVar):\n",
        "  HighDimVar=np.array(HighDimVar)\n",
        "  sum=0\n",
        "  #i,j = product\n",
        "  #return math.sqrt(i*i+j*j)\n",
        "  for i in range(len(HighDimVar)):\n",
        "    sum = sum + HighDimVar[i]**2\n",
        "  return math.sqrt(sum)\n",
        "\n",
        "\n",
        "def SqrtAllComp(HighDimVar):\n",
        "  HighDimVar=np.array(HighDimVar)\n",
        "  product=1\n",
        "\n",
        "  for i in range(len(HighDimVar)):\n",
        "    product=product * HighDimVar[i]\n",
        "\n",
        "  return math.sqrt(product)\n",
        "\n",
        "def IdentitySum(HighDimVar):\n",
        "  HighDimVar = np.array(HighDimVar)\n",
        "  sum = 0\n",
        "  for i in range(len(HighDimVar)):\n",
        "      sum = sum + HighDimVar[i]\n",
        "  return sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Fd2xwpNd2s"
      },
      "source": [
        "n Cantor sets (named as $A_1, A_2, A_3, A_4, A_5, ... A_n$ ) on [0,1] can be constructed so that their Cartesian product is reducible to a 1 dimensional set if $A_1$ only contains numbers with the digits 0 or 1, $A_2$ only contains numbers with the digits 0 or 2, $A_3$ only contains numbers with the digits 0 or 4,..... $A_n$ only contains numbers with the digits 0 or $2^\\text{n-1}$ in base $2^n$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0eeXEOyNfSk",
        "outputId": "0f3e5b2a-ba91-4f53-deab-f81d0c9eae54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.0078125, 0.03125, 0.0390625, 0.125, 0.1328125, 0.15625, 0.1640625, 0.5, 0.5078125, 0.53125, 0.5390625, 0.625, 0.6328125, 0.65625, 0.6640625], [0.0, 0.0009765625, 0.0078125, 0.0087890625, 0.0625, 0.0634765625, 0.0703125, 0.0712890625, 0.5, 0.5009765625, 0.5078125, 0.5087890625, 0.5625, 0.5634765625, 0.5703125, 0.5712890625], [0.0, 0.0001220703125, 0.001953125, 0.0020751953125, 0.03125, 0.0313720703125, 0.033203125, 0.0333251953125, 0.5, 0.5001220703125, 0.501953125, 0.5020751953125, 0.53125, 0.5313720703125, 0.533203125, 0.5333251953125], [0.0, 1.52587890625e-05, 0.00048828125, 0.0005035400390625, 0.015625, 0.0156402587890625, 0.01611328125, 0.0161285400390625, 0.5, 0.5000152587890625, 0.50048828125, 0.5005035400390625, 0.515625, 0.5156402587890625, 0.51611328125, 0.5161285400390625]]\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "x=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "def HighDimCantorSet(CantorDimension):\n",
        "  for i in range(2,CantorDimension+2):\n",
        "\n",
        "    xinAi(kgap,0,1,i)\n",
        "    xcopy=x.copy()\n",
        "    #print(x)\n",
        "    CantorList.append(xcopy)\n",
        "\n",
        "    x.clear()\n",
        "\n",
        "\n",
        "HighDimCantorSet(4)\n",
        "\n",
        "print(CantorList)\n",
        "\n",
        "\n",
        "CantorList=[]\n",
        "def HighDim(NumofCarProdut,function):  #NumofCarProdut, it makes sense for training purpose for a value larger than 2\n",
        "\n",
        "  #CantorList.clear()\n",
        "\n",
        "  #CantorListCopy=CantorList.copy()\n",
        "  HighDimCantorSet(NumofCarProdut)\n",
        "  CarProdut=list(itertools.product(*CantorList))\n",
        "\n",
        "  #CantorList.clear()\n",
        "\n",
        "  for i in range(len(CarProdut)):    # HighDimList list of high dim points on the cantor set x1,x2,x3,....xn in R^n\n",
        "      HighDimList.append(CarProdut[i])\n",
        "      #print(CarProdut)\n",
        "\n",
        "\n",
        "  for i in range(len(CarProdut)):    # sum of each point's coordinate and return a one dim list\n",
        "        #print(sum(CarProdut[i]))\n",
        "        oneDimlist.append(sum(CarProdut[i]))\n",
        "\n",
        "  for i in range(len(HighDimList)):   # the function which input is each point in the HighDimlist, the function we will test is identity sum, distance and sqrt\n",
        "      outputlist.append(function(HighDimList[i]))\n",
        "\n",
        "HighDim(3,IdentitySum) #two layer sin(a/100)/100 sin(a/1)/100\n",
        "\n",
        "#print(HighDimList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acd0BcoqW6sn"
      },
      "source": [
        "# Higher Dim Case(>=2)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baBkOTjHbo-Z"
      },
      "source": [
        " ##  x,y to mutivariate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK5ciEr5W7mH",
        "outputId": "60bb88d5-5cfb-4297-dada-3776af21ab85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.0, 0.0), (0.0, 0.0009765625), (0.0, 0.0078125), (0.0, 0.0087890625), (0.0, 0.0625), (0.0, 0.0634765625), (0.0, 0.0703125), (0.0, 0.0712890625), (0.0, 0.5), (0.0, 0.5009765625), (0.0, 0.5078125), (0.0, 0.5087890625), (0.0, 0.5625), (0.0, 0.5634765625), (0.0, 0.5703125), (0.0, 0.5712890625), (0.0078125, 0.0), (0.0078125, 0.0009765625), (0.0078125, 0.0078125), (0.0078125, 0.0087890625), (0.0078125, 0.0625), (0.0078125, 0.0634765625), (0.0078125, 0.0703125), (0.0078125, 0.0712890625), (0.0078125, 0.5), (0.0078125, 0.5009765625), (0.0078125, 0.5078125), (0.0078125, 0.5087890625), (0.0078125, 0.5625), (0.0078125, 0.5634765625), (0.0078125, 0.5703125), (0.0078125, 0.5712890625), (0.03125, 0.0), (0.03125, 0.0009765625), (0.03125, 0.0078125), (0.03125, 0.0087890625), (0.03125, 0.0625), (0.03125, 0.0634765625), (0.03125, 0.0703125), (0.03125, 0.0712890625), (0.03125, 0.5), (0.03125, 0.5009765625), (0.03125, 0.5078125), (0.03125, 0.5087890625), (0.03125, 0.5625), (0.03125, 0.5634765625), (0.03125, 0.5703125), (0.03125, 0.5712890625), (0.0390625, 0.0), (0.0390625, 0.0009765625), (0.0390625, 0.0078125), (0.0390625, 0.0087890625), (0.0390625, 0.0625), (0.0390625, 0.0634765625), (0.0390625, 0.0703125), (0.0390625, 0.0712890625), (0.0390625, 0.5), (0.0390625, 0.5009765625), (0.0390625, 0.5078125), (0.0390625, 0.5087890625), (0.0390625, 0.5625), (0.0390625, 0.5634765625), (0.0390625, 0.5703125), (0.0390625, 0.5712890625), (0.125, 0.0), (0.125, 0.0009765625), (0.125, 0.0078125), (0.125, 0.0087890625), (0.125, 0.0625), (0.125, 0.0634765625), (0.125, 0.0703125), (0.125, 0.0712890625), (0.125, 0.5), (0.125, 0.5009765625), (0.125, 0.5078125), (0.125, 0.5087890625), (0.125, 0.5625), (0.125, 0.5634765625), (0.125, 0.5703125), (0.125, 0.5712890625), (0.1328125, 0.0), (0.1328125, 0.0009765625), (0.1328125, 0.0078125), (0.1328125, 0.0087890625), (0.1328125, 0.0625), (0.1328125, 0.0634765625), (0.1328125, 0.0703125), (0.1328125, 0.0712890625), (0.1328125, 0.5), (0.1328125, 0.5009765625), (0.1328125, 0.5078125), (0.1328125, 0.5087890625), (0.1328125, 0.5625), (0.1328125, 0.5634765625), (0.1328125, 0.5703125), (0.1328125, 0.5712890625), (0.15625, 0.0), (0.15625, 0.0009765625), (0.15625, 0.0078125), (0.15625, 0.0087890625), (0.15625, 0.0625), (0.15625, 0.0634765625), (0.15625, 0.0703125), (0.15625, 0.0712890625), (0.15625, 0.5), (0.15625, 0.5009765625), (0.15625, 0.5078125), (0.15625, 0.5087890625), (0.15625, 0.5625), (0.15625, 0.5634765625), (0.15625, 0.5703125), (0.15625, 0.5712890625), (0.1640625, 0.0), (0.1640625, 0.0009765625), (0.1640625, 0.0078125), (0.1640625, 0.0087890625), (0.1640625, 0.0625), (0.1640625, 0.0634765625), (0.1640625, 0.0703125), (0.1640625, 0.0712890625), (0.1640625, 0.5), (0.1640625, 0.5009765625), (0.1640625, 0.5078125), (0.1640625, 0.5087890625), (0.1640625, 0.5625), (0.1640625, 0.5634765625), (0.1640625, 0.5703125), (0.1640625, 0.5712890625), (0.5, 0.0), (0.5, 0.0009765625), (0.5, 0.0078125), (0.5, 0.0087890625), (0.5, 0.0625), (0.5, 0.0634765625), (0.5, 0.0703125), (0.5, 0.0712890625), (0.5, 0.5), (0.5, 0.5009765625), (0.5, 0.5078125), (0.5, 0.5087890625), (0.5, 0.5625), (0.5, 0.5634765625), (0.5, 0.5703125), (0.5, 0.5712890625), (0.5078125, 0.0), (0.5078125, 0.0009765625), (0.5078125, 0.0078125), (0.5078125, 0.0087890625), (0.5078125, 0.0625), (0.5078125, 0.0634765625), (0.5078125, 0.0703125), (0.5078125, 0.0712890625), (0.5078125, 0.5), (0.5078125, 0.5009765625), (0.5078125, 0.5078125), (0.5078125, 0.5087890625), (0.5078125, 0.5625), (0.5078125, 0.5634765625), (0.5078125, 0.5703125), (0.5078125, 0.5712890625), (0.53125, 0.0), (0.53125, 0.0009765625), (0.53125, 0.0078125), (0.53125, 0.0087890625), (0.53125, 0.0625), (0.53125, 0.0634765625), (0.53125, 0.0703125), (0.53125, 0.0712890625), (0.53125, 0.5), (0.53125, 0.5009765625), (0.53125, 0.5078125), (0.53125, 0.5087890625), (0.53125, 0.5625), (0.53125, 0.5634765625), (0.53125, 0.5703125), (0.53125, 0.5712890625), (0.5390625, 0.0), (0.5390625, 0.0009765625), (0.5390625, 0.0078125), (0.5390625, 0.0087890625), (0.5390625, 0.0625), (0.5390625, 0.0634765625), (0.5390625, 0.0703125), (0.5390625, 0.0712890625), (0.5390625, 0.5), (0.5390625, 0.5009765625), (0.5390625, 0.5078125), (0.5390625, 0.5087890625), (0.5390625, 0.5625), (0.5390625, 0.5634765625), (0.5390625, 0.5703125), (0.5390625, 0.5712890625), (0.625, 0.0), (0.625, 0.0009765625), (0.625, 0.0078125), (0.625, 0.0087890625), (0.625, 0.0625), (0.625, 0.0634765625), (0.625, 0.0703125), (0.625, 0.0712890625), (0.625, 0.5), (0.625, 0.5009765625), (0.625, 0.5078125), (0.625, 0.5087890625), (0.625, 0.5625), (0.625, 0.5634765625), (0.625, 0.5703125), (0.625, 0.5712890625), (0.6328125, 0.0), (0.6328125, 0.0009765625), (0.6328125, 0.0078125), (0.6328125, 0.0087890625), (0.6328125, 0.0625), (0.6328125, 0.0634765625), (0.6328125, 0.0703125), (0.6328125, 0.0712890625), (0.6328125, 0.5), (0.6328125, 0.5009765625), (0.6328125, 0.5078125), (0.6328125, 0.5087890625), (0.6328125, 0.5625), (0.6328125, 0.5634765625), (0.6328125, 0.5703125), (0.6328125, 0.5712890625), (0.65625, 0.0), (0.65625, 0.0009765625), (0.65625, 0.0078125), (0.65625, 0.0087890625), (0.65625, 0.0625), (0.65625, 0.0634765625), (0.65625, 0.0703125), (0.65625, 0.0712890625), (0.65625, 0.5), (0.65625, 0.5009765625), (0.65625, 0.5078125), (0.65625, 0.5087890625), (0.65625, 0.5625), (0.65625, 0.5634765625), (0.65625, 0.5703125), (0.65625, 0.5712890625), (0.6640625, 0.0), (0.6640625, 0.0009765625), (0.6640625, 0.0078125), (0.6640625, 0.0087890625), (0.6640625, 0.0625), (0.6640625, 0.0634765625), (0.6640625, 0.0703125), (0.6640625, 0.0712890625), (0.6640625, 0.5), (0.6640625, 0.5009765625), (0.6640625, 0.5078125), (0.6640625, 0.5087890625), (0.6640625, 0.5625), (0.6640625, 0.5634765625), (0.6640625, 0.5703125), (0.6640625, 0.5712890625)]\n",
            "Epoch 1/5\n",
            "7/7 - 0s - loss: 0.0671 - 271ms/epoch - 39ms/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.0198 - 10ms/epoch - 1ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.0145 - 13ms/epoch - 2ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.0067 - 16ms/epoch - 2ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0040 - 12ms/epoch - 2ms/step\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Dimension:  2\n",
            "********** 0.052200858395147896\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "\n",
        "\n",
        "def increaseDiscreteDimMV(finalDim):  #it will generate a series of Cantor sets with increasing dimesntionality and the corresponding learning tasks\n",
        "  for i in range(2,finalDim+1):   #finalDim corresponds to the dimention of the last high dim Cantor set in the series of Cantor sets\n",
        "\n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "\n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,SqrtAllComp)\n",
        "\n",
        "    print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "\n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "\n",
        "    z1 = np.array(HighDimList)\n",
        "    k1 = np.array(outputlist)\n",
        "\n",
        "    #print(HighDimListCopy)\n",
        "\n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(100, input_dim=i, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(1)) # Output\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"Dimension: \", i )\n",
        "    print(\"**********\", score2)\n",
        "\n",
        "\n",
        "\n",
        "increaseDiscreteDimMV(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Spy7L2fBIgp"
      },
      "source": [
        "## Decomposition x+y to (x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VykBzWABJCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74b7fa5-9322-44df-f94c-4b19fa4afe15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7/7 - 0s - loss: 0.1767 - 282ms/epoch - 40ms/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.1041 - 13ms/epoch - 2ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.0668 - 13ms/epoch - 2ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.0473 - 13ms/epoch - 2ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0385 - 12ms/epoch - 2ms/step\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "********** 0.1910666752511418\n",
            "Epoch 1/5\n",
            "103/103 - 0s - loss: 0.0499 - 364ms/epoch - 4ms/step\n",
            "Epoch 2/5\n",
            "103/103 - 0s - loss: 0.0435 - 128ms/epoch - 1ms/step\n",
            "Epoch 3/5\n",
            "103/103 - 0s - loss: 0.0434 - 126ms/epoch - 1ms/step\n",
            "Epoch 4/5\n",
            "103/103 - 0s - loss: 0.0433 - 120ms/epoch - 1ms/step\n",
            "Epoch 5/5\n",
            "103/103 - 0s - loss: 0.0432 - 116ms/epoch - 1ms/step\n",
            "26/26 [==============================] - 0s 1ms/step\n",
            "********** 0.205223456072421\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "def increaseDiscreteDimDECOMP(finalDim):\n",
        "  for i in range(2,finalDim+1):\n",
        "\n",
        "\n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "\n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,IdentitySum)\n",
        "\n",
        "    #print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #Distencefunction\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "\n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "\n",
        "    z1 = np.array(oneDimlist)\n",
        "    k1 = np.array(HighDimList)\n",
        "\n",
        "    #print(HighDimListCopy)\n",
        "\n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(100, input_dim=1, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(i)) # Output dimension\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"**********\", score2)\n",
        "\n",
        "\n",
        "\n",
        "increaseDiscreteDimDECOMP(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swi2Bw3NCFJk"
      },
      "source": [
        "## Comparing with decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHTWoyZGCR4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c748b8-43cd-4b92-b506-d0837b86fa03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++++++ 204\n",
            "Epoch 1/5\n",
            "7/7 - 0s - loss: 0.6104 - 286ms/epoch - 41ms/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.3825 - 11ms/epoch - 2ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.2353 - 15ms/epoch - 2ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.1311 - 11ms/epoch - 2ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0567 - 13ms/epoch - 2ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "********** 0.1795708508382325\n",
            "+++++++ 3276\n",
            "Epoch 1/5\n",
            "103/103 - 0s - loss: 0.1511 - 406ms/epoch - 4ms/step\n",
            "Epoch 2/5\n",
            "103/103 - 0s - loss: 0.0011 - 105ms/epoch - 1ms/step\n",
            "Epoch 3/5\n",
            "103/103 - 0s - loss: 1.4572e-05 - 105ms/epoch - 1ms/step\n",
            "Epoch 4/5\n",
            "103/103 - 0s - loss: 8.9972e-06 - 115ms/epoch - 1ms/step\n",
            "Epoch 5/5\n",
            "103/103 - 0s - loss: 6.8520e-06 - 119ms/epoch - 1ms/step\n",
            "26/26 [==============================] - 0s 986us/step\n",
            "********** 0.0024738635029057173\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "def increaseDiscreteDimIDDECOMP(finalDim):\n",
        "  for i in range(2,finalDim+1):\n",
        "\n",
        "\n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "\n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,IdentitySum)\n",
        "\n",
        "\n",
        "    #print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "\n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "\n",
        "    z1 = np.array(HighDimList)\n",
        "    k1 = np.array(outputlist)\n",
        "\n",
        "\n",
        "    #print(HighDimListCopy)\n",
        "\n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "    print(\"+++++++\",len(z1_train))\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(100, input_dim=i, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(1)) # Output\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"**********\", score2)\n",
        "\n",
        "\n",
        "\n",
        "increaseDiscreteDimIDDECOMP(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Culi19sXPeND"
      },
      "source": [
        "## x+y to function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhIcSv-_WeVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb44993-4aba-46bc-9434-e539d6c4f81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.0, 0.0), (0.0, 0.0009765625), (0.0, 0.0078125), (0.0, 0.0087890625), (0.0, 0.0625), (0.0, 0.0634765625), (0.0, 0.0703125), (0.0, 0.0712890625), (0.0, 0.5), (0.0, 0.5009765625), (0.0, 0.5078125), (0.0, 0.5087890625), (0.0, 0.5625), (0.0, 0.5634765625), (0.0, 0.5703125), (0.0, 0.5712890625), (0.0078125, 0.0), (0.0078125, 0.0009765625), (0.0078125, 0.0078125), (0.0078125, 0.0087890625), (0.0078125, 0.0625), (0.0078125, 0.0634765625), (0.0078125, 0.0703125), (0.0078125, 0.0712890625), (0.0078125, 0.5), (0.0078125, 0.5009765625), (0.0078125, 0.5078125), (0.0078125, 0.5087890625), (0.0078125, 0.5625), (0.0078125, 0.5634765625), (0.0078125, 0.5703125), (0.0078125, 0.5712890625), (0.03125, 0.0), (0.03125, 0.0009765625), (0.03125, 0.0078125), (0.03125, 0.0087890625), (0.03125, 0.0625), (0.03125, 0.0634765625), (0.03125, 0.0703125), (0.03125, 0.0712890625), (0.03125, 0.5), (0.03125, 0.5009765625), (0.03125, 0.5078125), (0.03125, 0.5087890625), (0.03125, 0.5625), (0.03125, 0.5634765625), (0.03125, 0.5703125), (0.03125, 0.5712890625), (0.0390625, 0.0), (0.0390625, 0.0009765625), (0.0390625, 0.0078125), (0.0390625, 0.0087890625), (0.0390625, 0.0625), (0.0390625, 0.0634765625), (0.0390625, 0.0703125), (0.0390625, 0.0712890625), (0.0390625, 0.5), (0.0390625, 0.5009765625), (0.0390625, 0.5078125), (0.0390625, 0.5087890625), (0.0390625, 0.5625), (0.0390625, 0.5634765625), (0.0390625, 0.5703125), (0.0390625, 0.5712890625), (0.125, 0.0), (0.125, 0.0009765625), (0.125, 0.0078125), (0.125, 0.0087890625), (0.125, 0.0625), (0.125, 0.0634765625), (0.125, 0.0703125), (0.125, 0.0712890625), (0.125, 0.5), (0.125, 0.5009765625), (0.125, 0.5078125), (0.125, 0.5087890625), (0.125, 0.5625), (0.125, 0.5634765625), (0.125, 0.5703125), (0.125, 0.5712890625), (0.1328125, 0.0), (0.1328125, 0.0009765625), (0.1328125, 0.0078125), (0.1328125, 0.0087890625), (0.1328125, 0.0625), (0.1328125, 0.0634765625), (0.1328125, 0.0703125), (0.1328125, 0.0712890625), (0.1328125, 0.5), (0.1328125, 0.5009765625), (0.1328125, 0.5078125), (0.1328125, 0.5087890625), (0.1328125, 0.5625), (0.1328125, 0.5634765625), (0.1328125, 0.5703125), (0.1328125, 0.5712890625), (0.15625, 0.0), (0.15625, 0.0009765625), (0.15625, 0.0078125), (0.15625, 0.0087890625), (0.15625, 0.0625), (0.15625, 0.0634765625), (0.15625, 0.0703125), (0.15625, 0.0712890625), (0.15625, 0.5), (0.15625, 0.5009765625), (0.15625, 0.5078125), (0.15625, 0.5087890625), (0.15625, 0.5625), (0.15625, 0.5634765625), (0.15625, 0.5703125), (0.15625, 0.5712890625), (0.1640625, 0.0), (0.1640625, 0.0009765625), (0.1640625, 0.0078125), (0.1640625, 0.0087890625), (0.1640625, 0.0625), (0.1640625, 0.0634765625), (0.1640625, 0.0703125), (0.1640625, 0.0712890625), (0.1640625, 0.5), (0.1640625, 0.5009765625), (0.1640625, 0.5078125), (0.1640625, 0.5087890625), (0.1640625, 0.5625), (0.1640625, 0.5634765625), (0.1640625, 0.5703125), (0.1640625, 0.5712890625), (0.5, 0.0), (0.5, 0.0009765625), (0.5, 0.0078125), (0.5, 0.0087890625), (0.5, 0.0625), (0.5, 0.0634765625), (0.5, 0.0703125), (0.5, 0.0712890625), (0.5, 0.5), (0.5, 0.5009765625), (0.5, 0.5078125), (0.5, 0.5087890625), (0.5, 0.5625), (0.5, 0.5634765625), (0.5, 0.5703125), (0.5, 0.5712890625), (0.5078125, 0.0), (0.5078125, 0.0009765625), (0.5078125, 0.0078125), (0.5078125, 0.0087890625), (0.5078125, 0.0625), (0.5078125, 0.0634765625), (0.5078125, 0.0703125), (0.5078125, 0.0712890625), (0.5078125, 0.5), (0.5078125, 0.5009765625), (0.5078125, 0.5078125), (0.5078125, 0.5087890625), (0.5078125, 0.5625), (0.5078125, 0.5634765625), (0.5078125, 0.5703125), (0.5078125, 0.5712890625), (0.53125, 0.0), (0.53125, 0.0009765625), (0.53125, 0.0078125), (0.53125, 0.0087890625), (0.53125, 0.0625), (0.53125, 0.0634765625), (0.53125, 0.0703125), (0.53125, 0.0712890625), (0.53125, 0.5), (0.53125, 0.5009765625), (0.53125, 0.5078125), (0.53125, 0.5087890625), (0.53125, 0.5625), (0.53125, 0.5634765625), (0.53125, 0.5703125), (0.53125, 0.5712890625), (0.5390625, 0.0), (0.5390625, 0.0009765625), (0.5390625, 0.0078125), (0.5390625, 0.0087890625), (0.5390625, 0.0625), (0.5390625, 0.0634765625), (0.5390625, 0.0703125), (0.5390625, 0.0712890625), (0.5390625, 0.5), (0.5390625, 0.5009765625), (0.5390625, 0.5078125), (0.5390625, 0.5087890625), (0.5390625, 0.5625), (0.5390625, 0.5634765625), (0.5390625, 0.5703125), (0.5390625, 0.5712890625), (0.625, 0.0), (0.625, 0.0009765625), (0.625, 0.0078125), (0.625, 0.0087890625), (0.625, 0.0625), (0.625, 0.0634765625), (0.625, 0.0703125), (0.625, 0.0712890625), (0.625, 0.5), (0.625, 0.5009765625), (0.625, 0.5078125), (0.625, 0.5087890625), (0.625, 0.5625), (0.625, 0.5634765625), (0.625, 0.5703125), (0.625, 0.5712890625), (0.6328125, 0.0), (0.6328125, 0.0009765625), (0.6328125, 0.0078125), (0.6328125, 0.0087890625), (0.6328125, 0.0625), (0.6328125, 0.0634765625), (0.6328125, 0.0703125), (0.6328125, 0.0712890625), (0.6328125, 0.5), (0.6328125, 0.5009765625), (0.6328125, 0.5078125), (0.6328125, 0.5087890625), (0.6328125, 0.5625), (0.6328125, 0.5634765625), (0.6328125, 0.5703125), (0.6328125, 0.5712890625), (0.65625, 0.0), (0.65625, 0.0009765625), (0.65625, 0.0078125), (0.65625, 0.0087890625), (0.65625, 0.0625), (0.65625, 0.0634765625), (0.65625, 0.0703125), (0.65625, 0.0712890625), (0.65625, 0.5), (0.65625, 0.5009765625), (0.65625, 0.5078125), (0.65625, 0.5087890625), (0.65625, 0.5625), (0.65625, 0.5634765625), (0.65625, 0.5703125), (0.65625, 0.5712890625), (0.6640625, 0.0), (0.6640625, 0.0009765625), (0.6640625, 0.0078125), (0.6640625, 0.0087890625), (0.6640625, 0.0625), (0.6640625, 0.0634765625), (0.6640625, 0.0703125), (0.6640625, 0.0712890625), (0.6640625, 0.5), (0.6640625, 0.5009765625), (0.6640625, 0.5078125), (0.6640625, 0.5087890625), (0.6640625, 0.5625), (0.6640625, 0.5634765625), (0.6640625, 0.5703125), (0.6640625, 0.5712890625)]\n",
            "[0.0, 0.0009765625, 0.0078125, 0.0087890625, 0.0625, 0.0634765625, 0.0703125, 0.0712890625, 0.5, 0.5009765625, 0.5078125, 0.5087890625, 0.5625, 0.5634765625, 0.5703125, 0.5712890625, 0.0078125, 0.0087890625, 0.015625, 0.0166015625, 0.0703125, 0.0712890625, 0.078125, 0.0791015625, 0.5078125, 0.5087890625, 0.515625, 0.5166015625, 0.5703125, 0.5712890625, 0.578125, 0.5791015625, 0.03125, 0.0322265625, 0.0390625, 0.0400390625, 0.09375, 0.0947265625, 0.1015625, 0.1025390625, 0.53125, 0.5322265625, 0.5390625, 0.5400390625, 0.59375, 0.5947265625, 0.6015625, 0.6025390625, 0.0390625, 0.0400390625, 0.046875, 0.0478515625, 0.1015625, 0.1025390625, 0.109375, 0.1103515625, 0.5390625, 0.5400390625, 0.546875, 0.5478515625, 0.6015625, 0.6025390625, 0.609375, 0.6103515625, 0.125, 0.1259765625, 0.1328125, 0.1337890625, 0.1875, 0.1884765625, 0.1953125, 0.1962890625, 0.625, 0.6259765625, 0.6328125, 0.6337890625, 0.6875, 0.6884765625, 0.6953125, 0.6962890625, 0.1328125, 0.1337890625, 0.140625, 0.1416015625, 0.1953125, 0.1962890625, 0.203125, 0.2041015625, 0.6328125, 0.6337890625, 0.640625, 0.6416015625, 0.6953125, 0.6962890625, 0.703125, 0.7041015625, 0.15625, 0.1572265625, 0.1640625, 0.1650390625, 0.21875, 0.2197265625, 0.2265625, 0.2275390625, 0.65625, 0.6572265625, 0.6640625, 0.6650390625, 0.71875, 0.7197265625, 0.7265625, 0.7275390625, 0.1640625, 0.1650390625, 0.171875, 0.1728515625, 0.2265625, 0.2275390625, 0.234375, 0.2353515625, 0.6640625, 0.6650390625, 0.671875, 0.6728515625, 0.7265625, 0.7275390625, 0.734375, 0.7353515625, 0.5, 0.5009765625, 0.5078125, 0.5087890625, 0.5625, 0.5634765625, 0.5703125, 0.5712890625, 1.0, 1.0009765625, 1.0078125, 1.0087890625, 1.0625, 1.0634765625, 1.0703125, 1.0712890625, 0.5078125, 0.5087890625, 0.515625, 0.5166015625, 0.5703125, 0.5712890625, 0.578125, 0.5791015625, 1.0078125, 1.0087890625, 1.015625, 1.0166015625, 1.0703125, 1.0712890625, 1.078125, 1.0791015625, 0.53125, 0.5322265625, 0.5390625, 0.5400390625, 0.59375, 0.5947265625, 0.6015625, 0.6025390625, 1.03125, 1.0322265625, 1.0390625, 1.0400390625, 1.09375, 1.0947265625, 1.1015625, 1.1025390625, 0.5390625, 0.5400390625, 0.546875, 0.5478515625, 0.6015625, 0.6025390625, 0.609375, 0.6103515625, 1.0390625, 1.0400390625, 1.046875, 1.0478515625, 1.1015625, 1.1025390625, 1.109375, 1.1103515625, 0.625, 0.6259765625, 0.6328125, 0.6337890625, 0.6875, 0.6884765625, 0.6953125, 0.6962890625, 1.125, 1.1259765625, 1.1328125, 1.1337890625, 1.1875, 1.1884765625, 1.1953125, 1.1962890625, 0.6328125, 0.6337890625, 0.640625, 0.6416015625, 0.6953125, 0.6962890625, 0.703125, 0.7041015625, 1.1328125, 1.1337890625, 1.140625, 1.1416015625, 1.1953125, 1.1962890625, 1.203125, 1.2041015625, 0.65625, 0.6572265625, 0.6640625, 0.6650390625, 0.71875, 0.7197265625, 0.7265625, 0.7275390625, 1.15625, 1.1572265625, 1.1640625, 1.1650390625, 1.21875, 1.2197265625, 1.2265625, 1.2275390625, 0.6640625, 0.6650390625, 0.671875, 0.6728515625, 0.7265625, 0.7275390625, 0.734375, 0.7353515625, 1.1640625, 1.1650390625, 1.171875, 1.1728515625, 1.2265625, 1.2275390625, 1.234375, 1.2353515625]\n",
            "Epoch 1/5\n",
            "7/7 - 0s - loss: 0.0873 - 303ms/epoch - 43ms/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.0488 - 11ms/epoch - 2ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.0285 - 12ms/epoch - 2ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.0189 - 11ms/epoch - 2ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0148 - 11ms/epoch - 2ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Dimension:  2\n",
            "********** 0.11442219565736209\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "#def Pickhundredpt(list): #list size >=1000\n",
        " # listcopy=random.sample(len(list), 2)\n",
        " # return listcopy\n",
        "\n",
        "#test=[(1,2),(3,4),(3,7),(4,5)]\n",
        "\n",
        "#print(Pickhundredpt(test))\n",
        "\n",
        "\n",
        "def increaseDiscreteDimFRACTALMV(finalDim):\n",
        "  for i in range(2,finalDim+1):\n",
        "\n",
        "\n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "\n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,SqrtAllComp)\n",
        "\n",
        "\n",
        "    #print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "\n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "\n",
        "    print(HighDimList)\n",
        "    print(oneDimlist)\n",
        "\n",
        "    z1 = np.array(oneDimlist)\n",
        "    k1 = np.array(outputlist)\n",
        "\n",
        "\n",
        "    #print(HighDimListCopy)\n",
        "\n",
        "\n",
        "\n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(100, input_dim=1, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(1)) # Output\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"Dimension: \" ,i)\n",
        "    print(\"**********\", score2)\n",
        "\n",
        "\n",
        "\n",
        "increaseDiscreteDimFRACTALMV(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36rwMIxXoLGH"
      },
      "source": [
        "## HIGHER NEURON TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfUfjln-oJSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397527ee-d0dc-4b6e-b1d9-fac258e0a927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256\n",
            "Epoch 1/20\n",
            "7/7 - 0s - loss: 0.0919 - 357ms/epoch - 51ms/step\n",
            "Epoch 2/20\n",
            "7/7 - 0s - loss: 0.0432 - 44ms/epoch - 6ms/step\n",
            "Epoch 3/20\n",
            "7/7 - 0s - loss: 0.0353 - 37ms/epoch - 5ms/step\n",
            "Epoch 4/20\n",
            "7/7 - 0s - loss: 0.0332 - 39ms/epoch - 6ms/step\n",
            "Epoch 5/20\n",
            "7/7 - 0s - loss: 0.0322 - 38ms/epoch - 5ms/step\n",
            "Epoch 6/20\n",
            "7/7 - 0s - loss: 0.0325 - 44ms/epoch - 6ms/step\n",
            "Epoch 7/20\n",
            "7/7 - 0s - loss: 0.0338 - 49ms/epoch - 7ms/step\n",
            "Epoch 8/20\n",
            "7/7 - 0s - loss: 0.0349 - 39ms/epoch - 6ms/step\n",
            "Epoch 9/20\n",
            "7/7 - 0s - loss: 0.0325 - 48ms/epoch - 7ms/step\n",
            "Epoch 10/20\n",
            "7/7 - 0s - loss: 0.0328 - 48ms/epoch - 7ms/step\n",
            "Epoch 11/20\n",
            "7/7 - 0s - loss: 0.0322 - 43ms/epoch - 6ms/step\n",
            "Epoch 12/20\n",
            "7/7 - 0s - loss: 0.0321 - 41ms/epoch - 6ms/step\n",
            "Epoch 13/20\n",
            "7/7 - 0s - loss: 0.0326 - 42ms/epoch - 6ms/step\n",
            "Epoch 14/20\n",
            "7/7 - 0s - loss: 0.0320 - 36ms/epoch - 5ms/step\n",
            "Epoch 15/20\n",
            "7/7 - 0s - loss: 0.0321 - 37ms/epoch - 5ms/step\n",
            "Epoch 16/20\n",
            "7/7 - 0s - loss: 0.0325 - 43ms/epoch - 6ms/step\n",
            "Epoch 17/20\n",
            "7/7 - 0s - loss: 0.0324 - 38ms/epoch - 5ms/step\n",
            "Epoch 18/20\n",
            "7/7 - 0s - loss: 0.0325 - 41ms/epoch - 6ms/step\n",
            "Epoch 19/20\n",
            "7/7 - 0s - loss: 0.0319 - 39ms/epoch - 6ms/step\n",
            "Epoch 20/20\n",
            "7/7 - 0s - loss: 0.0320 - 43ms/epoch - 6ms/step\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "********** 0.18835084807885463\n"
          ]
        }
      ],
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "HighDimList=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "CantorList=[]\n",
        "\n",
        "def increaseDiscreteDimDECOMPHIGH(finalDim):\n",
        "  for i in range(2,finalDim+1):\n",
        "\n",
        "\n",
        "    oneDimlist.clear()\n",
        "    HighDimList.clear()\n",
        "    outputlist.clear()\n",
        "\n",
        "    CantorList.clear()\n",
        "\n",
        "    HighDim(i,Distencefunction)\n",
        "\n",
        "\n",
        "    #print(HighDimList)\n",
        "    #print(CarProdut)\n",
        "\n",
        "    #HighDim(xaxis,i,IdentitySum)\n",
        "\n",
        "    #HighDim(xaxis,i,SqrtAllComp)\n",
        "\n",
        "\n",
        "    z1 = np.array(oneDimlist)\n",
        "    k1 = np.array(HighDimList)\n",
        "\n",
        "    print(len(z1))\n",
        "    #print(HighDimListCopy)\n",
        "\n",
        "\n",
        "\n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(1000, input_dim=1, activation='relu')) # Hidden 1  dim increases in each loop no need for too many units  3000\n",
        "    modelh.add(Dense(500, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(150, activation='relu')) # Hidden 3\n",
        "    modelh.add(Dense(50, activation='relu')) # Hidden 4\n",
        "    modelh.add(Dense(i)) # Output dimension\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=20)\n",
        "\n",
        "    pred2 = modelh.predict(z1_test)\n",
        "    score2 = np.sqrt(metrics.mean_squared_error(pred2,k1_test))\n",
        "    print(\"**********\", score2)\n",
        "\n",
        "\n",
        "\n",
        "increaseDiscreteDimDECOMPHIGH(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now We turn our attention to the timeseries forcasting"
      ],
      "metadata": {
        "id": "DUbJ_ZDSZEwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain = []\n",
        "\n",
        "for i in range(0,800):\n",
        "  domain.append(i/1000)\n",
        "\n",
        "test_domain = []\n",
        "\n",
        "for i in range(800,1001):\n",
        "  test_domain.append(i/1000)\n",
        "\n",
        "def timeseries(f):\n",
        "  series = []\n",
        "  for i in domain:\n",
        "    series.append(f(i))\n",
        "  return series\n",
        "\n"
      ],
      "metadata": {
        "id": "y_wz2o1DZLv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 3\n",
        "\n",
        "#returns the RMSE, percent of variance explained and the L2 norm of the data\n",
        "def function_info (function):\n",
        "  #model for the data\n",
        "  tensorflow.random.set_seed(seed)\n",
        "  model1 = Sequential()\n",
        "  model1.add(Dense(3000, input_dim=1, activation='relu')) # Hidden 1\n",
        "  model1.add(Dense(1500, activation='relu')) # Hidden 2\n",
        "  model1.add(Dense(1)) # Output\n",
        "  model1.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  #fits the model based on the input function\n",
        "  model1.fit(domain,timeseries(function),verbose=2,epochs=16)\n",
        "\n",
        "\n",
        "  #processes the data for the model\n",
        "\n",
        "  pred1 = model1.predict(test_domain)\n",
        "  y1 = np.array(list(map(function, test_domain)))\n",
        "\n",
        "  #score1 = np.sqrt(metrics.mean_squared_error(pred1, y1)) #we shall try both the mean sqaure error or just norm\n",
        "  score1 = norm(pred1-y1,2)/(len(y1))\n",
        "  pvariance1 = (y1.std()-score1)/y1.std()\n",
        "\n",
        "  #calculates the l2 norm\n",
        "\n",
        "  sum = 0\n",
        "  n = len(y1)\n",
        "  for i in range (n-1):\n",
        "    sum += (y1[i+1]-y1[i])**2\n",
        "\n",
        "  sum = sum/n\n",
        "\n",
        "  sum = np.sqrt(sum)\n",
        "\n",
        "  return score1, pvariance1, sum\n"
      ],
      "metadata": {
        "id": "pu5YmyydZVaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function_array = []\n",
        "\n",
        "for i in range(1,11):\n",
        "  f = lambda x, i=i : (np.sin(1/x))*(x**(1+(1/i))) if x!=0 else 0\n",
        "  function_array.append(f)\n",
        "\n",
        "for i in function_array:\n",
        "  print(i(1/2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kmFi_YJZYc6",
        "outputId": "e082ff4c-7abd-44f9-d4a6-53af77d25cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22732435670642043\n",
            "0.32148518831195905\n",
            "0.36085492297376603\n",
            "0.38231247330857127\n",
            "0.3957946935634067\n",
            "0.4050459559836544\n",
            "0.41178609866514904\n",
            "0.4169147084373769\n",
            "0.42094774672290075\n",
            "0.4242022491739435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_those_results = np.array(list(map(function_info, function_array)))\n",
        "all_those_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqELpCRWZbfD",
        "outputId": "99bfa984-c118-46e6-dfc2-bc75354ff6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0103 - 908ms/epoch - 36ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 3.8586e-04 - 626ms/epoch - 25ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 6.8691e-05 - 603ms/epoch - 24ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 2.4801e-05 - 603ms/epoch - 24ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 1.7839e-05 - 627ms/epoch - 25ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 1.4265e-05 - 609ms/epoch - 24ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 1.3147e-05 - 598ms/epoch - 24ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 1.2319e-05 - 620ms/epoch - 25ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 1.1605e-05 - 612ms/epoch - 24ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 1.0866e-05 - 592ms/epoch - 24ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 1.0309e-05 - 618ms/epoch - 25ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 9.9064e-06 - 592ms/epoch - 24ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 9.8489e-06 - 597ms/epoch - 24ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 9.7491e-06 - 603ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 9.6679e-06 - 609ms/epoch - 24ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 9.5605e-06 - 604ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0134 - 852ms/epoch - 34ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0013 - 591ms/epoch - 24ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 6.6322e-04 - 591ms/epoch - 24ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 4.3293e-04 - 596ms/epoch - 24ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 2.5134e-04 - 592ms/epoch - 24ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 1.7202e-04 - 595ms/epoch - 24ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 1.5219e-04 - 619ms/epoch - 25ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 1.3664e-04 - 593ms/epoch - 24ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 1.2655e-04 - 608ms/epoch - 24ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 1.1411e-04 - 602ms/epoch - 24ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 1.1021e-04 - 709ms/epoch - 28ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 1.0497e-04 - 697ms/epoch - 28ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 1.0544e-04 - 603ms/epoch - 24ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 1.0785e-04 - 592ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 1.0513e-04 - 622ms/epoch - 25ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 9.5901e-05 - 610ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0156 - 813ms/epoch - 33ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0021 - 605ms/epoch - 24ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0013 - 595ms/epoch - 24ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 8.5715e-04 - 597ms/epoch - 24ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 5.2244e-04 - 617ms/epoch - 25ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 3.8925e-04 - 595ms/epoch - 24ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 3.3521e-04 - 591ms/epoch - 24ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 2.9773e-04 - 614ms/epoch - 25ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 2.5056e-04 - 594ms/epoch - 24ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 2.2133e-04 - 595ms/epoch - 24ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 2.0951e-04 - 603ms/epoch - 24ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 2.0753e-04 - 601ms/epoch - 24ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 2.0716e-04 - 610ms/epoch - 24ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 2.0581e-04 - 613ms/epoch - 25ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 1.8950e-04 - 618ms/epoch - 25ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 1.6865e-04 - 590ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 6ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0170 - 848ms/epoch - 34ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0027 - 640ms/epoch - 26ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0017 - 601ms/epoch - 24ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0011 - 587ms/epoch - 23ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 7.0228e-04 - 598ms/epoch - 24ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 5.8356e-04 - 596ms/epoch - 24ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 4.9850e-04 - 608ms/epoch - 24ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 3.8712e-04 - 612ms/epoch - 24ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 3.5056e-04 - 605ms/epoch - 24ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 3.3020e-04 - 615ms/epoch - 25ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 3.1850e-04 - 603ms/epoch - 24ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 2.9474e-04 - 607ms/epoch - 24ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 2.6347e-04 - 603ms/epoch - 24ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 2.4014e-04 - 597ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 2.3672e-04 - 625ms/epoch - 25ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 2.3954e-04 - 587ms/epoch - 23ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0180 - 833ms/epoch - 33ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0032 - 585ms/epoch - 23ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0020 - 598ms/epoch - 24ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0013 - 593ms/epoch - 24ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 8.8079e-04 - 613ms/epoch - 25ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 7.2938e-04 - 606ms/epoch - 24ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 5.9359e-04 - 593ms/epoch - 24ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 4.6444e-04 - 633ms/epoch - 25ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 4.1587e-04 - 880ms/epoch - 35ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 4.0286e-04 - 595ms/epoch - 24ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 3.8321e-04 - 594ms/epoch - 24ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 3.4698e-04 - 624ms/epoch - 25ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 3.0789e-04 - 590ms/epoch - 24ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 2.9331e-04 - 594ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 2.9943e-04 - 595ms/epoch - 24ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 3.1657e-04 - 603ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0187 - 844ms/epoch - 34ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0035 - 603ms/epoch - 24ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0022 - 613ms/epoch - 25ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0014 - 614ms/epoch - 25ms/step\n",
            "25/25 - 1s - loss: 0.0010 - 846ms/epoch - 34ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 8.8230e-04 - 824ms/epoch - 33ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 6.6459e-04 - 589ms/epoch - 24ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 5.2514e-04 - 606ms/epoch - 24ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 4.6549e-04 - 590ms/epoch - 24ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 4.4181e-04 - 607ms/epoch - 24ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 4.0310e-04 - 655ms/epoch - 26ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 3.5978e-04 - 603ms/epoch - 24ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 3.3371e-04 - 607ms/epoch - 24ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 3.3114e-04 - 597ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 3.3899e-04 - 613ms/epoch - 25ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 3.7697e-04 - 599ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0193 - 837ms/epoch - 33ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0038 - 604ms/epoch - 24ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0024 - 614ms/epoch - 25ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0015 - 601ms/epoch - 24ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0012 - 600ms/epoch - 24ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 9.8910e-04 - 608ms/epoch - 24ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 7.5444e-04 - 608ms/epoch - 24ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 5.7709e-04 - 633ms/epoch - 25ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 5.1642e-04 - 599ms/epoch - 24ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 4.9991e-04 - 616ms/epoch - 25ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 4.7252e-04 - 613ms/epoch - 25ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 4.2843e-04 - 612ms/epoch - 24ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 3.9024e-04 - 598ms/epoch - 24ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 3.7417e-04 - 601ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 3.6884e-04 - 600ms/epoch - 24ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 3.8191e-04 - 606ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0198 - 864ms/epoch - 35ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0040 - 619ms/epoch - 25ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0025 - 613ms/epoch - 25ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0016 - 618ms/epoch - 25ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0012 - 618ms/epoch - 25ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 0.0011 - 600ms/epoch - 24ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 8.3284e-04 - 609ms/epoch - 24ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 6.1065e-04 - 641ms/epoch - 26ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 5.5302e-04 - 637ms/epoch - 25ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 5.3337e-04 - 608ms/epoch - 24ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 5.0103e-04 - 623ms/epoch - 25ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 4.5327e-04 - 619ms/epoch - 25ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 4.1959e-04 - 616ms/epoch - 25ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 4.0409e-04 - 604ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 3.9615e-04 - 615ms/epoch - 25ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 4.1875e-04 - 605ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 8ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0201 - 845ms/epoch - 34ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0042 - 613ms/epoch - 25ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0026 - 616ms/epoch - 25ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0017 - 594ms/epoch - 24ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0013 - 609ms/epoch - 24ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 0.0011 - 611ms/epoch - 24ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 8.9989e-04 - 595ms/epoch - 24ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 6.4928e-04 - 638ms/epoch - 26ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 5.9193e-04 - 603ms/epoch - 24ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 5.7508e-04 - 597ms/epoch - 24ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 5.5448e-04 - 597ms/epoch - 24ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 5.1348e-04 - 613ms/epoch - 25ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 4.6608e-04 - 600ms/epoch - 24ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 4.3478e-04 - 609ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 4.1398e-04 - 611ms/epoch - 24ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 4.0764e-04 - 603ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n",
            "Epoch 1/16\n",
            "25/25 - 1s - loss: 0.0204 - 856ms/epoch - 34ms/step\n",
            "Epoch 2/16\n",
            "25/25 - 1s - loss: 0.0043 - 594ms/epoch - 24ms/step\n",
            "Epoch 3/16\n",
            "25/25 - 1s - loss: 0.0028 - 614ms/epoch - 25ms/step\n",
            "Epoch 4/16\n",
            "25/25 - 1s - loss: 0.0018 - 619ms/epoch - 25ms/step\n",
            "Epoch 5/16\n",
            "25/25 - 1s - loss: 0.0014 - 594ms/epoch - 24ms/step\n",
            "Epoch 6/16\n",
            "25/25 - 1s - loss: 0.0012 - 620ms/epoch - 25ms/step\n",
            "Epoch 7/16\n",
            "25/25 - 1s - loss: 8.8856e-04 - 624ms/epoch - 25ms/step\n",
            "Epoch 8/16\n",
            "25/25 - 1s - loss: 6.6518e-04 - 621ms/epoch - 25ms/step\n",
            "Epoch 9/16\n",
            "25/25 - 1s - loss: 5.9885e-04 - 603ms/epoch - 24ms/step\n",
            "Epoch 10/16\n",
            "25/25 - 1s - loss: 5.7016e-04 - 617ms/epoch - 25ms/step\n",
            "Epoch 11/16\n",
            "25/25 - 1s - loss: 5.3306e-04 - 613ms/epoch - 25ms/step\n",
            "Epoch 12/16\n",
            "25/25 - 1s - loss: 4.8374e-04 - 597ms/epoch - 24ms/step\n",
            "Epoch 13/16\n",
            "25/25 - 1s - loss: 4.4438e-04 - 615ms/epoch - 25ms/step\n",
            "Epoch 14/16\n",
            "25/25 - 1s - loss: 4.2812e-04 - 598ms/epoch - 24ms/step\n",
            "Epoch 15/16\n",
            "25/25 - 1s - loss: 4.2282e-04 - 617ms/epoch - 25ms/step\n",
            "Epoch 16/16\n",
            "25/25 - 1s - loss: 4.5738e-04 - 609ms/epoch - 24ms/step\n",
            "7/7 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8.09561515e-02, -1.92323876e-01,  1.16781785e-03],\n",
              "       [ 8.77624335e-02, -8.65546317e-01,  8.12144048e-04],\n",
              "       [ 9.07066408e-02, -1.29270072e+00,  6.84911854e-04],\n",
              "       [ 9.51335992e-02, -1.66340371e+00,  6.19586555e-04],\n",
              "       [ 9.70725793e-02, -1.90825848e+00,  5.79838875e-04],\n",
              "       [ 9.96773278e-02, -2.13414914e+00,  5.53110150e-04],\n",
              "       [ 9.05647596e-02, -1.95269544e+00,  5.33905755e-04],\n",
              "       [ 9.01426747e-02, -2.02296999e+00,  5.19441214e-04],\n",
              "       [ 8.38711749e-02, -1.87685370e+00,  5.08154879e-04],\n",
              "       [ 9.29909913e-02, -2.24916353e+00,  4.99103143e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = []\n",
        "var = []\n",
        "for i in all_those_results:\n",
        "  rmse, pove, l2 = i\n",
        "  loss.append(rmse)\n",
        "  var.append(l2)\n",
        "\n",
        "nploss = np.array(loss)\n",
        "npvar = np.array(var)\n",
        "\n",
        "plt.scatter(npvar,nploss)\n",
        "a, b = np.polyfit(npvar, nploss, 1)\n",
        "plt.plot(npvar, a*npvar + b)\n",
        "plt.xlabel(\"Complexity\")\n",
        "plt.ylabel(\"Loss of the model\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "le61RjWhZeo3",
        "outputId": "2d6a8f1e-8091-4f71-e851-05ebf0c8ccac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEGCAYAAADFWoruAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hV9Z3+/fdNOAVUEASEBAqeUBQFErGt1lqtBW0riKFifVptnTodaw/OjB2Z/qYHZzra8pupbe0zHWs91KdVCyLSqqUqrae2asJRUBDxQAJyFDwFSeDz/LFWcBMD2Rx29t7J/bquXNn7u9da+excmpu11nd/P4oIzMzMCkGnfBdgZmbWxKFkZmYFw6FkZmYFw6FkZmYFw6FkZmYFo3O+C8inww47LIYOHZrvMszMikpNTc2GiOiXi2N36FAaOnQo1dXV+S7DzKyoSHolV8f25TszMysYDiUzMysYOQ0lSeMlLZO0QtI1Lbx+uqR5kholVTV77RJJL6Rfl2SMV0hanB7zJ5KUjveR9FC6/UOSDs3lezMzswMvZ6EkqQT4GXAOMAK4SNKIZpu9ClwK/KbZvn2A7wCnAGOB72SEzP8AXwKOTr/Gp+PXAI9ExNHAI+lzMzMrIrk8UxoLrIiIlRGxDbgLmJC5QUS8HBGLgB3N9h0HPBQRmyLideAhYLykgcAhEfG3SBbt+xUwMd1nAnB7+vj2jHEzMysSuZx9VwasynheS3Lms6/7lqVftS2MAwyIiDXp49eAAS0dWNLlwOUAQ4YMybKctjVrfh3T5ixj9eZ6BvUu5epxw5k4uqz1Hc3Mily7nOiQnkW1uPx5RNwUEZURUdmvX06m2e+XWfPrmDpzMXWb6wmgbnM9U2cuZtb8unyXZmaWc7kMpTpgcMbz8nRsf/atSx+3dMy16eU90u/r9qHmvJs2Zxn1Ddt3Gatv2M60OcvyVJGZWdvJZSg9AxwtaZikrsAUYHaW+84BPiHp0HSCwyeAOenluTckfTCddfd54L50n9lA0yy9SzLGi8rqzfV7NW5m1p7kLJQiohG4kiRgngN+GxFLJF0r6TwASSdLqgUmA/8raUm67ybg30mC7Rng2nQM4ArgZmAF8CLwYDp+PXC2pBeAj6fPi86g3qV7NW5m1p6oI3eeraysjEJbZqjpnlLmJbzSLiVcN2mkJzuYWUGQVBMRlbk4dode+64QNQWPZ9+ZWUfkUCpAE0eXOYTMrENql1PCzcysODmUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYDiUzMysYOQ0lCSNl7RM0gpJ17TwejdJd6evPyVpaDreVdKtkhZLWijpjHT8YEkLMr42SLohfe1SSeszXvu7XL43MzM78HLW5E9SCfAz4GygFnhG0uyIWJqx2WXA6xFxlKQpwA+AC4EvAUTESEn9gQclnRwRbwKjMn5GDTAz43h3R8SVuXpPZmaWW7k8UxoLrIiIlRGxDbgLmNBsmwnA7enjGcBZkgSMAOYCRMQ6YDOwSz94SccA/YHHc/YOzMysTeUylMqAVRnPa9OxFreJiEZgC9AXWAicJ6mzpGFABTC42b5TSM6MImPsAkmLJM2Q1Hx7ACRdLqlaUvX69ev39b2ZmVkOFOpEh1tIQqwauAH4C7C92TZTgDsznv8OGBoRJwIP8d4Z2C4i4qaIqIyIyn79+h3wws3MbN/l7J4SUMeuZzfl6VhL29RK6gz0AjamZz9XNW0k6S/A8oznJwGdI6KmaSwiNmYc92bghwfofRS8WfPrmDZnGas31zOodylXjxvOxNHNT0rNzApfLs+UngGOljRMUleSM5vZzbaZDVySPq4C5kZESOohqSeApLOBxmYTJC5i17MkJA3MeHoe8NyBeyuFa9b8OqbOXEzd5noCqNtcz9SZi5k1v3n+m5kVvpydKUVEo6QrgTlACXBLRCyRdC1QHRGzgV8Cd0haAWwiCS5IJjDMkbSD5Gzqc80O/xng3GZjX5N0HtCYHuvSHLytgjNtzjLqG3a9slnfsJ1pc5b5bMnMik4uL98REQ8ADzQb+3bG463A5Bb2exkYvofjHtHC2FRg6n6UW5RWb67fq3Ezs0JWqBMdLEuDepfu1biZWSFzKBW5q8cNp7RLyS5jpV1KuHrcbk80zcwKVk4v31nuNd038uw7M2sPHErtwMTRZQ4hM2sXfPnOzMwKhkPJzMwKhkPJzMwKhkPJzMwKhkPJzMwKhkPJzMwKhkPJzMwKhkPJzMwKhkPJzMwKhkPJzMwKhkPJzMwKhkPJzMwKhhdktZ1mza/zauNmllc5PVOSNF7SMkkrJF3TwuvdJN2dvv6UpKHpeFdJt0paLGmhpDMy9vlzeswF6Vf/PR3LsjNrfh1TZy6mbnM9AdRtrmfqzMXMml+X79LMrAPJWShJKgF+BpwDjAAukjSi2WaXAa9HxFHAj4AfpONfAoiIkcDZwH9Jyqz14ogYlX6ta+VYloVpc5ZR37B9l7H6hu1Mm7MsTxWZWUeUyzOlscCKiFgZEduAu4AJzbaZANyePp4BnCVJJCE2FyANnc1AZSs/b3fHsiys3ly/V+NmZrmQy1AqA1ZlPK9Nx1rcJiIagS1AX2AhcJ6kzpKGARXA4Iz9bk0v3f1bRvDs7liWhUG9S/dq3MwsFwp19t0tJCFWDdwA/AVourZ0cXpZ7yPp1+f25sCSLpdULal6/fr1B7Dk4nb1uOGUdinZZay0SwlXjxuep4rMrCPKZSjVsevZTXk61uI2kjoDvYCNEdEYEVel94wmAL2B5QARUZd+fxP4Dcllwt0eq3lREXFTRFRGRGW/fv0OyBttDyaOLuO6SSMp612KgLLepVw3aaRn35lZm8rllPBngKPTy291wBTgs822mQ1cAvwVqALmRkRI6gEoIt6WdDbQGBFL07DpHREbJHUBPgU8vKdj5fD9tTsTR5c5hMwsr3IWShHRKOlKYA5QAtwSEUskXQtUR8Rs4JfAHZJWAJtIggugPzBH0g6SQGu6RNctHe+SHvNh4Bfpa7s7lpmZFQl15JOJysrKqK6uzncZZmZFRVJNRLQ2I3qfFOpEBzMz64AcSmZmVjB2e09J0qQ97RgRMw98OWZm1pHtaaLDp/fwWgAOJTMzO6B2G0oR8YW2LMTMzKzVe0qSBkj6paQH0+cjJF2W+9LMzKyjyWaiw20knzUalD5fDnwjVwWZmVnHlU0oHRYRvwV2wM7FTrfveRczM7O9l00ovS2pL8nkBiR9kGQFbjMzswMqm2WG/pFkXbkjJT0J9CNZW87MzOyAajWUImKepI8CwwEByyKiIeeVmZlZh7MvH549RlKH/vDs6s31vLrpHcYO7UOnTm5ua2Z2oGTz4dn+wIdJ25MDHyNputdhQ+mup1/lJ3NXMKRPDy4YU84FFWWUH9oj32WZmRW9Vj88K+mPwIiIWJM+H0gyTbxDmjW/juk1tQCsfWMrP3p4OT96eDkfPrIvkyvLGX/8QEq7lrRyFDMza0k2Ex0GNwVSai0wJEf1FLRZ8+uYOnMx9Q3JjPh3G3fQrXMnPnZsf5as3sJVdy/k292W8KmTBlJVMZgxQ3oj+fKemVm2sgmlRyTNAe5Mn1/Ie91eO5Rpc5btDKQm7zbuYHHtFh7/5sd46qVNTK9Zxaz5q7nz6VUc0a8nVRXlXDCmnAGHdM9T1WZmxSOrJn+SzgdOT58+FhH35rSqNrK3Tf6GXXM/Lf22BLx0/Sd3Pn/r3UYeWLSG6TWreObl1+kkOP2YflRVlPPx4wbQvYsv75lZ8cplk79s26H/BWgk+QDt07kopBgM6l1K3eb6FsczHdStM585eTCfOXkwL214m3tqarlnXi1X/mY+vUq7cN5Jg5hcWc7Isl6+vGdmliGbBVk/QxJEVcBngKckZfXhWUnjJS2TtELSNS283k3S3enrT0kamo53lXSrpMWSFko6Ix3vIel+Sc9LWiLp+oxjXSppvaQF6dffZVPj3rh63HBKm53llHYp4epxw3e7z7DDevLP44bzxL+cyR2XjeWjx/Tjt9WrOO/GJxl/w+Pc/PhK1r/57oEu1cysKLV6+U7SQuDsiFiXPu8HPBwRJ7WyXwnJ4q1nA7XAM8BFEbE0Y5srgBMj4suSpgDnR8SFkr4CVEbEFyT1Bx4ETga6A6dExJ8kdQUeAf4zIh6UdGm6z5XZvvm9vXwHyWSHaXOWsXpzPYN6l3L1uOFMHF22V8fYUt/A7xetZnp1LQtWbaZzJ3HG8P5MriznY8P707WzGwKbWeHK9+W7Tk2BlNpIdmvmjQVWRMRKAEl3AROApRnbTAC+mz6eAdyo5HrWCNLPRUXEOkmbSQLnaeBP6fg2SfOA8ixqOWAmji7b6xBqrldpFy4+5QNcfMoHeGHtm8yoqWXm/Doefm4tfXt2ZcKoMiZXlnPcwEMOUNVmZsUhm3D5g6Q56eWxS4H7Sc5cWlMGrMp4XpuOtbhNuvr4FqAvsBA4T1JnScOACmBw5o6SepN8wPeRjOELJC2SNEPSLttn7He5pGpJ1evXr8/ibeTW0QMOZuq5x/HXa87klksrGTusD3f87WXO+fHjfOqnj3Pbky/x+tvb8l2mmVmbyGbtu6slXQCcmg7d1Aaz724BjgOqgVdIJlrsnIstqTPJFPWfNJ2JAb8D7oyIdyX9PXA7cGbzA0fETcBNkFy+y+Wb2BudSzpx5rEDOPPYAWx6exuzFyQf0v3u75bynw88z8dH9GdyxWA+cvRhdC7x5T0za5+ymn0XEfdIeqhpe0l9ImJTK7vVsevZTXk61tI2tWnQ9AI2RnKj66qmjST9heT+VJObgBci4oaMGjdmvH4z8MNs3lsh6tOzK5eeOoxLTx3G0tVvML1mFfctWM0Di1+j/8HdOH9MGZMrBnNU/4PyXaqZ2QHVaiilZx3fA7aSNPoTydTwI1rZ9Rng6PTyWx0wBfhss21mA5cAfyWZ3Tc3IkJSD5JJGG9LOhtobJogIek/SMJrl9l1kgZmrDxxHvBca++tGIwYdAjfGXQ8U885jrnPr2NGzSpufvwl/vfRlYwe0puqinI+fdIgDuneJd+lmpntt2xm370AfCgiNuz1waVzgRuAEuCWiPi+pGuB6oiYLak7cAcwGtgETImIlenU8DkkIVgHXBYRr0gqJ7kH9TzQNI/6xoi4WdJ1JGHUmB7rHyLi+T3Vty+z7wrB+jffTdfgW8XytW/RrXMnxp9wOFUV5Zx65GFeudzMciqXs++yCaU/AJMi4p1cFJBPxRpKTSKCRbVbmFFTy30L6nhjayODenXngopyqirK+UDfnvku0czaoXyH0mjgVuAp3js7ISK+louC2lKxh1KmrQ3beWjpWmbU1PL4C+vZETB2aB+qKsv55MiB9OyW7eIdZmZ7lu9Qehp4AlhMcjkNgIi4PRcFtaX2FEqZXtuylXvm1XJPTS0rN7xNj64lnDtyIFUV5ZwyrI+XNjKz/ZLvUJofEaNz8cPzrb2GUpOIYN6rrzO9upbfL1rDW+82MqRPD6oqypk0xo0JzWzf5DuU/hN4meRzQJmX71qbEl7w2nsoZXpnWyNzlrzG9Opa/vLiRiSSxoQVgxl3/OFuTGhmWct3KL3UwnBERGtTwgteRwqlTKs2vcPMeXXMmLeKVZvqObhbZzcmNLOs5TWU2rOOGkpNduwInnppEzNqanlg8RrqG7a7MaGZtcqhlCMdPZQy7a4x4eSKwXx8RH+6dfblPTNLOJRyxKHUspc3vM2MtDHhmi1b6VXahQmjBjG5YjAnlB3iy3tmHZxDKUccSnu2fUfwlxc3ML26lj8seY1tjTs49vCDqaooZ+LoMg47qFu+SzSzPMj3RAcBFwNHRMS1koYAh6e9jYqaQyl7W+ob+N3C1cyoea8x4ceO7U9VRTlnHtufLl653KzDyHco/Q/Jh2bPjIjjJB0K/DEiTs5FQW3JobRvMhsTrn/zXfr27MrE0UljwmMP3/vGhAeim6+ZtZ18h9K8iBiT+SFaSQtba4deDPIRSu3pD3Dj9h089sJ6plfX8vBza2nYHpxQdgiTKwYzYdQgevfo2uoxZs2vY+rMxdQ37GyXRWmXEq6bNLJofy9m7V2+26E3SCohaVeBpH5kLDdk2Wv+B7hucz1TZy4GKMo/wM0bE963oI4ZNbV8Z/YSvn//c5w9YgBVFeV7bEw4bc6yXQIJoL5hO9PmLCvK34mZ7Z9sQuknwL1Af0nfJ+l79H9yWlU71Z7/APfp2ZUvnDqMLzRrTHj/4jX0P7gbk8YkK5c3b0y4enN9i8fb3biZtW/ZtEP/taQa4CySBn8TI6JdNNBrax3lD3BLjQl/8fhKfv7oi4we0pvJFYP51EkDOaR7Fwb1LqWuhfc/qHdpHio3s3zLtp/BC8AbvNcOfUhEvJqzqtqpjvYHuGvafHD8CYez7s2t3Dd/NdNrVvGv9y7me79bwvgTDue8kwZx65MvsbXxvSvCpV1KuHrc8DxWbmb5ks1Eh68C3wHWAttJ26FHxIm5Ly+3cjnRoaUJDUCHv6nf1Jhwes0qZi9YzRtbG+ndowvbtwdvvttIWZFP/jDrCHI50SGbD5d8HRgeEcdHxIkRMTLbQJI0XtIySSskXdPC690k3Z2+/lTaBh1JXSXdKmmxpIWSzsjYpyIdXyHpJ+nnqJDUR9JDkl5Ivx+aTY250DShoW5zPcGuExqumzSSst6lCCjrXdqhAglAEicN7s1/TBzJ09/6OD+9aDQnlvfmrW2NAJQdWkrD9h28/W5jnis1s3zI5kzpT8DZEbFXfyXSGXvLgbOBWuAZ4KKIWJqxzRXAiRHxZUlTgPMj4kJJXwEqI+ILkvoDDwInR8SOtOng10g64T4A/CQiHpT0Q2BTRFyfBuChEfEve6oxV2dKp14/t8XLdGW9S3nymjMP+M9rD9ZsqU9WLq+p5aWMxoSTK8oZ68aEZgUlL1PCJf1j+nAl8GdJ97NrP6X/buXYY4EVEbEyPd5dwARgacY2E4Dvpo9nADemZz4jgLnpz1knaTNQKWkVcEhE/C095q+AiSShNQE4Iz3W7cCfgT2GUq50lAkNB9LAXqV85WNHccUZR1LzyuvMqEkaE86oqd3ZmPCCinLK2un9NzNL7Gmiw8Hp91fTr67pF6SfWWpFGbAq43ktcMrutomIRklbgL7AQuA8SXcCg4GK9PuO9DiZx2y69jUgItakj18DBrRUlKTLgcsBhgwZksXb2HsdbULDgSSJyqF9qBzah29/egR/eDZpTPjfDy3nRw8v59QjD6OqotyNCc3aqd2GUkR8D0DS5IiYnvmapMk5rusW4DigGngF+AvJJIusRERIajE4I+Im4CZILt/tf6nvd/W44S1OaPCMsr3To2tnJo0pZ9KYclZteod75tUyo6aWb9y9IG1MOIjJleWMHuzGhGbtRTZTwqcC07MYa66O5OymSXk61tI2tZI6A72AjZHc6LqqaSNJfyG5P/V6epyWjrlW0sCIWCNpILCutTeWK00TF9rLckKFYHCfHnzj48fwtTOP5m8vbWRGTS2z5tdx59OvcmS/nlRVDGbSmDI3JjQrcrud6CDpHOBc4DPA3RkvHQKMiIixezxwEjLLST50W0cy0eGzEbEkY5uvACMzJjpMiojPSOqR1va2pLOBf4uI09N9mk90+GlEPCBpGkmgNU106BMR39xTjV6Qtbi9ubWBBxYn953cmNCs7eRlQVZJJwGjgGuBb2e89Cbwp4h4vdWDS+cCNwAlwC0R8X1J1wLVETFbUnfgDmA0sAmYEhEr06nhc0juIdUBl0XEK+kxK4HbgFKSCQ5fTS/X9QV+CwwhueT3mYjYtKf6HErtx0sb3mZGzSpmzqtjzZat9O7RhQknDaLKjQnNDrh8rxLeJSIacvHD882h1P5s3xE8uWID02tqmePGhGY54c6zOeJQat+2vNPA7xatZnpNLQszGhNOrijnY25MaLbPHEo54lDqOJY3NSacV8eGt/a/MaFZR5ave0p3RMTnJH09In6cix+ebw6ljqdx+w4eXZ40Jnzk+aQx4ciyXlRVlGfdmNCso8tXKC0FPk4ymeAMkoVYd2ptEkExcCh1bE2NCadX17J0zRt0LemUNCasLOcjR+2+MaFZR5evUPoa8A/AESQz4DJDKSLiiFwU1JYcStZkyeotOz/79Po7DQw4pBvnjy5ncmU5R/Y7qPUDmHUg+Z599z8R8Q+5+OH55lCy5rY17mDu82uZUVPLn5atZ/uOYMyQ3lRlNCY06+jyPtEh/czSR9Knj0XEolwU09YcSrYn697cyqz5yeW9F9a9RfcunRh//OFUVQzmw0f2pVMnf/bJOqZ8nyl9jWQB05np0PnATRHx01wU1JYcSpaNlhoTlvUu5YIxZVRVDGZI3x75LtGsTeU7lBYBH4qIt9PnPYG/uvOsdURbG7bz0NK1TK+p5fEX1hMBY4f1YXJFOeeOHEjPbtksJ2lW3PIdSotJGuxtTZ93B56JiJG5KKgtOZRsf7gxoXVU+Q6lfwQuAe5NhyYCt0XEDbkoqC05lOxAiAhqXnmd6dW1/H7Rat7etp0P9O1B1ZhyJrkxobVDhTDRYQxwWvr08YiYn4ti2ppDyQ60d7Y17mxM+NeVG5Hg1CMPY3Jl0piwexevXG7FL++h1F45lCyXMhsT1r5e78aE1m44lHLEoWRtYceO2NmY8MHFr1HfsN2NCa2oOZRyxKFkba2pMeH06lqqX0kaE370mH5MrhzMWce5MaEVh1yGUqvzV9Mp4PURsUPSMcCxwIPttceSWa7Mml/HtDnLWL25nkG9S/nWucexuX4b99TUccWv5+1sTDi5cjDHD3JjQuuYspl9V0OymsOhwJMkbc23RcTFuS8vt3ymZG1l1vw6ps5cTH3D9p1jpV1KuG7SSD590iCeWLGBGW5MaEUi31PC50XEGElfBUoj4oeSFkTEqFwU1JYcStZWTr1+LnWb6983Xta7lCevOXPn8y3vNDB70WpmuDGhFbBchlI2/4VL0oeAi4H707GsLnxLGi9pmaQVkq5p4fVuku5OX39K0tB0vIuk2yUtlvScpKnp+HBJCzK+3pD0jfS170qqy3jt3GxqNGsLq1sIpJbGe/Xowuc++AHu+8qp/PGq0/niacOY/+pmLr+jhg9d9wj/8fulPP/aG21RslleZLMmyjeAqcC9EbFE0hHAn1rbSVIJ8DPgbKAWeEbS7IhYmrHZZcDrEXGUpCnAD4ALgclAt4gYKakHsFTSnRGxDBiVcfw63vtQL8CPIuL/ZvGezNrUoN6lLZ4pDdrDB2uPGXAw/3rucXxz3PCdjQlv/+vL3PzES4ws68XkynLOO8mNCa19aTWUIuJR4FEASZ2ADRHxtSyOPRZYEREr033vAiYAmaE0Afhu+ngGcKOSu7sB9JTUGSgFtgHN/3l4FvBiRLySRS1meXX1uOEt3lO6etzwVvftXNKJs44bwFnHDdilMeG371vCf/z+uZ2NCU8/uh8lXrncilw2s+9+A3wZ2E4yyeEQST+OiGmt7FoGrMp4XgucsrttIqJR0hagL0lATQDWAD2Aq1rodDsFuLPZ2JWSPg9UA/8UEa+38H4uJ1n1nCFDhrTyFswOjImjywB2mX139bjhO8ez1adnV75w6jC+cOowlqzewvTqWu5bUMf9i9e4MaG1C9lMdFgQEaMkXQyMAa4BalpbJVxSFTA+Iv4uff454JSIuDJjm2fTbWrT5y+SBNdw4ArgUpJZf48D52ScdXUFVgPHR8TadGwAsIHkLOvfgYER8cU91eiJDtYeNDUmnF5dy5+Xv9eYcHLlYD55ohsT2oGX188pAV0kdSFZiPXGiGiQlM0nbuuAwRnPy9OxlrapTS/V9QI2Ap8F/pB+FmqdpCeBSmBlut85wLymQALIfCzpF8Dvs6jRrOh17dyJ8ScMZPwJA3dpTDh15mK+97sljD/+cCZXDuZDR7gxoRW+bELpf4GXgYXAY5I+wPvv77TkGeBoScNIwmcKSdhkmk2yAvlfgSpgbkSEpFeBM4E70g/vfhDIXJX8IppdupM0MCLWpE/PB57NokazdqX/wd25/PQj+dJHjmBh7RZmpI0JZy1Y7caEVhT2aZkhSZ0jojGL7c4lCZMS4JaI+L6ka4HqiJid9ma6AxgNbAKmRMRKSQcBtwIjAAG3Nt3DSkPqVeCIiNiS8bPuIJmZFyQh+vcZIdUiX76zjmBrw3b+uHQtMzIaE54yrA9Vbkxo+yjfH57tBXwHOD0dehS4NjMQipVDyTqapsaE06tX8fLGd+jRtYRPjhxIlRsT2l7IdyjdQ3Ip7PZ06HPASRExKRcFtSWHknVUbkxo+yPfofS+JYW8zJBZ++HGhLa38j37rl7SaRHxRFrMqUDLa6aYWdHp0bUzk8aUM2lMOas2vcOMmqQx4dfvWuDGhNbmsjlTOgn4Fcl0bYDXgUsiYlGOa8s5nymZtWxnY8LqWh54dg1bG3ZwVP+DqKoo5/zRbkzY0RVEkz9JhwBExBuSvhERN7S2T6FzKJm1zo0JrbmCCKVddpJejYiiX6PHoWS2d1auf4t75tVyT00dr72x1Y0JO6hCDKVVETG49S0Lm0PJbN9s3xG7bUx4/ugy+roxYbtWiKHkMyUzAzIaE1avYmHtFjp3Emce25/JlYM5Y3g/NyZsh/ISSpLeJFkd4X0vkXSgLfqPgTuUzA6s5WvfZEZNLTPn1bHhrXc57KCuTBxVxuTKwQw//OB8l2cHSMGdKbUXDiWz3GjYvoNHl61nRk0tjzy/lobt4caE7YhDKUccSma5t/Gtd7lvwWqm19Ty3Jo36FrSyY0Ji5xDKUccSmZtK7Mx4evvNDDgkG5MGlNOVYUbExYTh1KOOJTM8mNPjQk/deJADnZjwoLmUMoRh5JZ/q17Yyv3zq9jek0tK9a9RfcundyYsMA5lHLEoWRWOCKChbVbmF69itkLV/Pm1kY3JixQDqUccSiZFaamxoTTq1fxxIoNOxsTTq4czDknHO7GhHnmUMoRh5JZ4Vu9uT65vJc2JuzZtYRzR3mFVa0AABB0SURBVA5kcuVgTh56qJc2yoOiDSVJ44Efk7RDvzkirm/2ejeSFcgrgI3AhRHxsqQuwM3AGJL2Gr+KiOvSfV4G3gS2A41NvxhJfYC7gaEk7dA/ExGv76k+h5JZ8YgIql95nRktNCa8oKKcQW5M2GaKMpQklQDLgbOBWuAZ4KKIWJqxzRXAiRHxZUlTgPMj4kJJnwXOi4gpknoAS4Ez0sB6GaiMiA3Nft4PgU0Rcb2ka4BDI+Jf9lSjQ8msOL2zrZEHF7/G9JpV/G3lJiQ47ajDqKpwY8K2kO8mf/tqLLAiIlYCSLoLmEASME0mAN9NH88AblRyLh5AT0mdgVJgG/BGKz9vAnBG+vh24M/AHkPJzIpTj66duaAiOUN6X2PC7p359EmDqKpwY8JilMtQKgNWZTyvBU7Z3TYR0ShpC9CXJKAmAGuAHsBVEbEp3SeAP0oK4H8j4qZ0fEBErEkfvwYMaKkoSZcDlwMMGVL0a8qadXiD+/TgqrOP4etnHc3fVm5M196r5TdPvbqzMeGk0WX0d2PColCoU1jGktwzGgQcCjwu6eH0rOu0iKiT1B94SNLzEfFY5s4REWlovU8aYjdBcvkup+/CzNpMp07iw0cdxoePOozvTTie+xetYXpNLdc/+Dw//MPzbkxYJHIZSnVAZs+l8nSspW1q00t1vUgmPHwW+ENENADrJD0JVAIrI6IOICLWSbqXJMAeA9ZKGhgRayQNBNbl8L2ZWQE7uHsXpowdwpSxQ1i5/q2dK5df8et59O7RhYmjyqiqKHdjwgKUy0YnzwBHSxomqSswBZjdbJvZwCXp4ypgbiQzL14FzgSQ1BP4IPC8pJ6SDs4Y/wTwbAvHugS4LyfvysyKyhH9DuKb44/lyWvO5PYvjuW0ow7jN0+/yqd++gTn/PhxfvnES2x86918l2mpXE8JPxe4gWRK+C0R8X1J1wLVETFbUnfgDmA0sAmYEhErJR0E3AqMIOnfdGtETJN0BHBvevjOwG8i4vvpz+oL/BYYArxCMiV8E3vg2XdmHZMbE+6fopwSXgwcSma27LU3mVGzinvn17HhrW1uTJgFh1KOOJTMrElTY8LpNat45Ll1NO4ITizvRVWFGxM251DKEYeSmbWkxcaExw9gckU5H3FjQodSrjiUzKw1z9ZtYUaNGxNmcijlyIEIpVnz65g2ZxmrN9czqHcpV48bzsTRZQeoQjMrFO82bmfuc+uYXlPLox28MaFDKUf2N5Rmza9j6szF1Dds3zlW2qWE6yaNdDCZtWMdvTGhQylH9jeUTr1+LnWb6983Xta7lCevOXN/SjOzIrDbxoQV5VSNKW+3jQmLdUHWdm91C4G0p3Eza18kMWpwb0YN7s2/fWoEc5a8xoyaWn469wV+8sgLOxsTnjvycHp09Z/bbPi3tB8G9S5t8UzJfV3MOp7uXUqYMKqMCaPKWL25npnzkpXL/3n6Qr5z37NuTJglX77zPSUzy5GmxoTTq1dx/6I1vL1tO0P79khWLh9TvI0JfU8pRzz7zszaSntqTOhQyhF/TsnM8uHVje9wT3p5r25z/c7GhJMryhlVBI0JHUo54lAys3zasSP428qNTK+p5cFn17C1YUdRNCZ0KOWIQ8nMCsWbWxt2NiaseeV1SjqJjx7Tj6qK8oJrTOhQyhGHkpkVoszGhK+9sXWXxoQnlPXKd3kOpVxxKJlZIdu+I3j8hfXMqKnlj0vXsq1xB8cNPISqinImjhpE34O65aUuh1KOOJTMrFhseaeB2QuTpY0W5bkxoUMpRxxKZlaMWmpMeP7oMqoq2qYxYdGGkqTxwI9J2qHfHBHXN3u9G/AroALYCFwYES9L6gLcDIwhWXXiVxFxnaTB6fYDgABuiogfp8f6LvAlYH16+H+NiAf2VJ9DycyK2e4aE06uKOfTOWxMWJShJKkEWA6cDdQCzwAXRcTSjG2uAE6MiC9LmgKcHxEXSvoscF5ETJHUA1gKnAG8CwyMiHmSDgZqgIkRsTQNpbci4v9mW6NDyczai7ZsTFisC7KOBVZExEoASXcBE0gCpskE4Lvp4xnAjUo+NRZAT0mdgVJgG/BGRGwC1gBExJuSngPKmh3TzKzD6XtQN7542jC+eNqwnY0JZy2o4/5Fazj8kO5MGpPM3juiwBsT5jKUyoBVGc9rgVN2t01ENEraAvQlCagJJAHUA7gqDaSdJA0FRgNPZQxfKenzQDXwTxHx+oF6M2ZmxeKEsl6cUNaLqeceu7Mx4c8ffZH/988vUvGBQ6mqKC/YxoRtN11j74wFtgODgGHAP0k6oulFSQcB9wDfiIg30uH/AY4ERpGE2X+1dGBJl0uqllS9fv36ljYxM2sXunUu4ZyRA7nl0pP529SzmHrOsWypb2DqzMXc9fSq1g+QB7k8U6oDBmc8L0/HWtqmNr1U14tkwsNngT9ERAOwTtKTQCWwMp0EcQ/w64iY2XSgiFjb9FjSL4Dft1RURNwE3ATJPaX9eodmZkWi/yHd+fuPHsnlpx/BwtotDOlTmA0Ic3mm9AxwtKRhkroCU4DZzbaZDVySPq4C5kYy8+JV4EwAST2BDwLPp/ebfgk8FxH/nXkgSQMznp4PPHuA34+ZWdFrakzYp2duZubtr5ydKaX3iK4E5pBMCb8lIpZIuhaojojZJAFzh6QVwCaS4AL4GXCrpCWAgFsjYpGk04DPAYslLUi3bZr6/UNJo0gmSbwM/H2u3puZmeWGPzzrKeFmZnsll1PCC3Wig5mZdUAOJTMzKxgOJTMzKxgOJTMzKxgOJTMzKxgOJTMzKxgOJTMzKxgOJTMzKxi5XPvOzMwKzKz5dUybs4zVm+sZ1LuUq8cNZ+LosnyXtZNDycysg5g1v46pMxdT37AdgLrN9UyduRigYILJl+/MzDqIaXOW7QykJvUN25k2Z1meKno/h5KZWQexenP9Xo3ng0PJzKyDGNS7dK/G88GhZGbWQVw9bjilXUp2GSvtUsLV44bnqaL380QHM7MOomkyg2ffmZlZQZg4uqygQqg5X74zM7OC4VAyM7OC4VAyM7OC4VAyM7OC4VAyM7OCoYjIdw15I2k98Mp+HOIwYMMBKqetFGPNUJx1F2PNUJx1F2PNUJx1Hwb0jIh+uTh4hw6l/SWpOiIq813H3ijGmqE46y7GmqE46y7GmqE46851zb58Z2ZmBcOhZGZmBcOhtH9uyncB+6AYa4birLsYa4birLsYa4birDunNfuekpmZFQyfKZmZWcFwKJmZWcHo0KEkabykZZJWSLqmhde7Sbo7ff0pSUMzXpuaji+TNK61Y0q6TdJLkhakX6OKpG5J+r6k5ZKek/S1Iqj58Yzf82pJs/al5jzUfZakeWndT0g6qghqPjOt+VlJt0va584DOar7FknrJD3b7Fh9JD0k6YX0+6FFUPNkSUsk7ZC0X1Oy27juaZKel7RI0r2Seu+xuIjokF9ACfAicATQFVgIjGi2zRXAz9PHU4C708cj0u27AcPS45Ts6ZjAbUBVEdb9BeBXQKf0ef9Cr7nZce8BPl8kv+vlwHEZx72tkGsm+UftKuCYdP9rgcsK5XedvnY6MAZ4ttmxfghckz6+BvhBEdR8HDAc+DNQWUh/Q1qp+xNA5/TxD1r7XXfkM6WxwIqIWBkR24C7gAnNtpkA3J4+ngGcJUnp+F0R8W5EvASsSI+XzTGLre5/AK6NiB0AEbGuCGoGQNIhwJnAvp4ptXXdARySPu4FrC7wmvsC2yJieXqsh4AL9qHmXNVNRDwGbGrh52Ue63ZgYqHXHBHPRcSyfagz33X/MSIa06d/A8r3VFxHDqUykn/lNalNx1rcJv2lbiH5H3F3+7Z2zO+np7A/ktStSOo+ErhQUrWkByUdXQQ1N5kIPBIRb+xDzfmo+++AByTVAp8Dri/wmjcAnTMuJVUBg/eh5lzVvScDImJN+vg1YEAR1Hyg5LPuLwIP7mmDjhxKbW0qcCxwMtAH+Jf8lpO1bsDWSJYV+QVwS57r2RsXAXfmu4i9cBVwbkSUA7cC/53nevYokusxU4AfSXoaeBPYnt+q9l76PvzZmByT9C2gEfj1nrbryKFUx67/qitPx1rcJr2B2wvYuId9d3vMiFgTiXdJ/uCMLYa6Sf4lNDN9fC9wYhHUjKTDSH7H9+9DvW1et6R+wEkR8VQ6fjfw4UKuGSAi/hoRH4mIscBjJPfF9kUu6t6TtZIGpscaCOzLZem2rvlAafO6JV0KfAq4OP1HwO7t682yYv8COgMrSW7WNd3sO77ZNl9h15t9v00fH8+uN/tWktw83O0xgYHpdwE3ANcXSd3XA19MH58BPFPoNaf7fRm4vVj+G0nHN/DepIHLgHsKueZ0n/7p927AI8CZhfK7zthvKO+/+T6NXSc6/LDQa8547c/s30SHtv5djweWAv2yqm9//qct9i/gXJJ/2b0IfCsduxY4L33cHZhOcjPvaeCIjH2/le63DDhnT8dMx+cCi4Fngf8POKhI6u5NcraxGPgryb/mC7rm9LU/A+OL7L+R89Pf88K0/iOKoOZpwHPp9t8owN/1ncAaoIHkrP+ydLwvSYi+ADwM9CmCms9Pn78LrAXmFMnvegXJfagF6dfP91SblxkyM7OC0ZHvKZmZWYFxKJmZWcFwKJmZWcFwKJmZWcFwKJmZWcFwKJntBUmHS7pL0ouSaiQ9IOmYHP2sMyT9fh/3/bKkz6ePL5U06MBWZ5Yb+7zMvFlHky5IeS/Jh3KnpGMnkaybtq8rGeRERPw84+mlJJ+P25cFXs3alM+UzLL3MaAh8w9+RCwEnkh7xjwrabGkC2Hnmc6jku6TtFLS9ZIulvR0ut2R6Xa3Sfp5uujtckmfav6DJfVM+9U8LWm+pAnp+I8lfTt9PE7SY5I6SfqupH+WVAVUAr9W0qfpk8roLyXpbEn35vKXZrY3fKZklr0TgJoWxicBo4CTgMOAZyQ9lr52EkkfnE0kS7LcHBFjJX0d+CrwjXS7oSRr9R0J/Envb/D3LWBuRHwxbZL2tKSHSRb6fUbS48BPSBZ03ZGc1EFEzJB0JfDPEVGdnu39l6R+EbGepF9WMS2ya+2cz5TM9t9pwJ0RsT0i1gKPkqwGD8lagWsiWYj3ReCP6fhikiBq8tuI2BERL5CE17HNfsYngGskLSBZgqg7MCQi3gG+RNLL6MaIeHFPhUayhMsdwP+ThtuHaKWVgFlb8pmSWfaWkPQM2hvvZjzekfF8B7v+/9d8va/mzwVcEC03eRtJsoJztpMZbgV+B2wFpsd7DdjM8s5nSmbZmwt0k3R504CkE4HNJI0QS9IWFKeTLGK5Nyan94KOJGlT3Tx85gBfTS+/IWl0+v0DwD8Bo4FzJJ3SwrHfBA5uehIRq0kmPfwfkoAyKxgOJbMspZe+zgc+nk4JXwJcB/wGWESyuvdc4JsR8dpeHv5VkiB7EPhyRGxt9vq/A12ARenP/fc0oH5Jcr9oNUm7i5sldW+2723Az9OJDqXp2K+BVRHx3F7WaZZTXiXcLM8k3Qb8PiJmtOHPvBGYHxG/bKufaZYN31My62Ak1QBvk1z2MysoPlMyM7OC4XtKZmZWMBxKZmZWMBxKZmZWMBxKZmZWMBxKZmZWMP5/+5SihqQaeP0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}